\chapter{Logistic Regression}

\section*{Introduction}
	本章内容主要来自于个人YY

\section{Sigmoid函数}
	\boldmath  %公式加粗
	sigmoid 函数的主要公式如下所示：
	
	\begin{equation}
		p(x) = \frac{1}{1+e^{-x}}
	\end{equation}		
	
	该公式主要用在二分类的问题中作为最后的预测结果分类，或者作为激活函数在神经网络中使用。
	
	为什么要使用sigmoid？暂时不清楚
	

\section{Logistic Regression}
	\boldmath  %公式加粗
	逻辑回归主要用在二分类问题中，最后给出该样本属于正类或者负类的概率，公式如下所示：
	
	\begin{equation}
		J(\theta) = - \frac{1}{m} \left[ \sum_{i=1}^{m} y^{(i)} log h_\theta(x^{(i)})+ (1-y^{(i)})log(1-h_\theta(x^{(i)})) \right]	
	
	\end{equation}
	
	通俗的来说，LR就是用来将两堆数据分来
	
	LR优点:
	
	\begin{itemize}
		\item 预测结果是介于0和1之间的概率
		\item 容易使用和解释
		\item 预测速度较快，计算量小
	\end{itemize}
	
	缺点：
	\begin{itemize}
		\item 当特征空间很大时，LR的性能不是很好，不能处理大量的特征
		\item 容易欠拟合
		\item 不能处理线性不可分的数据
		\item 对于非线性特征，需要进行转换
	\end{itemize}










	% \begin{enumerate}		
	% 	\item start with weights $w_i = \frac{1}{N} \quad i=1...N$
	% 	\item repeat for m=1 to M
	% 		\begin{itemize}
	% 			\item 使用输入数据训练一个分类器$G_m(x) \in (-1,1)$
	% 			\item 计算误差err:
	% 				\begin{equation*}
	% 					E_w[I_(y_i\not \equiv G_m(x_i))]=\frac{\sum_{i=1}^{N}w_i I_(y_i\not \equiv G_m(x_i))}{\sum_{i=1}^{N}w_i}
	% 				\end{equation*}
	% 			\item 输出$\alpha_m = \frac{1}{2}log(\frac{1-err}{err})$,从这里可以看出分类器的误差越大，权值越小
	% 			\item set $w_i = \frac{w_i}{Z_m} \exp(\alpha_m I_(y\not \equiv G_m(x))$，其中$Z_m$是规范化因子,\newline
	% 			$Z_m=\sum_{i=1}^{N}w_i \exp(-\alpha_m y_i G_m(x_i))$如果一个数据点分类错了，那么给这个点的权值大一点。
	% 		\end{itemize}
	% 	\item 这样我们就得到了$G(x)=sign[\sum_{m=1}^{M}\alpha_m G_m(x)]$
	% \end{enumerate}



%--------------------------------------------------------------------------------
%--------------------------------------------------------------------------------
%--------------------------------------------------------------------------------