\chapter{Logistic Regression}

\section*{Introduction}
	本章内容主要来自于个人YY

\section{Sigmoid函数}
	\boldmath  %公式加粗
	sigmoid 函数的主要公式如下所示：
	
	\begin{equation}
		p(x) = \frac{1}{1+e^{-x}}
	\end{equation}		
	
	该公式主要用在二分类的问题中作为最后的预测结果分类，或者作为激活函数在神经网络中使用。
	
	为什么要使用sigmoid？
	
	由于sigmoid的良好性质，求导容易，反向传播速度快，输出全部在0-1之间，永远为正，严格递增
	
	缺点：在中心点变化快，比较敏感
	

\section{Logistic Regression}
	\boldmath  %公式加粗
	逻辑回归主要用在二分类问题中，最后给出该样本属于正类或者负类的概率。决策函数就是sigmoid函数，就是说sigmoid的输出就是LR的输出，简化的公式如下：
	
	\begin{equation}
		h(x) = \frac{1}{1+e^{-\theta ^T x}}
	\end{equation}
	
	对于一个二分类问题来说，分类为1的概率就是h(x),而分类为0的概率为1-h(x)，因此综合起来可以得到LR的决策函数为(y=0时前半部分不起作用，y=1时后半部分不起作用，因此其实就是把分类为0和1的概率合起来写了)：
	
	\begin{equation}
		p(y|x;\theta) = h(x)^y [1-h(x)]^{1-y}
	\end{equation}
	
	对参数w做最大似然估计，我们可以得到似然函数：
	
	\begin{equation}
		\prod_{i=1}^m P(y_i|x_i;\theta) = \prod_{i=1}^m h(x_i)^{y_i}(1-h(x_i))^{1-y_i}
	\end{equation}
	
	对上式取对数得到
	
	\begin{equation}
		L(\theta) = \sum_{i=1}^{m} \left[ y^{(i)} log h_\theta(x^{(i)})+ (1-y^{(i)})log(1-h_\theta(x^{(i)}))\right]
	
	\end{equation}
	
	当我们使用最大似然估计的时候，目的就是最大化似然函数，但是当我们在机器学习的算法中，我们往往需要最小化一个函数来方便我们的计算，因此我们只需要对上式取负号即可得到LR的损失函数，乘以$\frac{1}{m}$是为了方便计算。
	
	\begin{equation}
		J(\theta) = - \frac{1}{m} \sum_{i=1}^{m} \left[ y^{(i)} log h_\theta(x^{(i)})+ (1-y^{(i)})log(1-h_\theta(x^{(i)})) \right]	
	
	\end{equation}
	
	通俗的来说，LR就是用来将两堆数据分来。对于LR的输出，如果输出的值越大表明结果属于这一类的概率会越大，但是输出的结果并不是代表着概率。
	
	LR优点:
	
	\begin{itemize}
		\item 预测结果是介于0和1之间的概率
		\item 容易使用和解释
		\item 预测速度较快，计算量小
	\end{itemize}
	
	缺点：
	\begin{itemize}
		\item 当特征空间很大时，LR
		\item 容易欠拟合的性能不是很好
		\item 对于线性不可分的数据，需要定义非线性函数对特征进行映射
		\item 预测结果呈S型，中间的概率变化很大，很敏感
	\end{itemize}


\section{一些其他的问题}
	1. LR是基于概率的一个模型，所有的点都会参与模型参数的更新。
	   SVM是基于最大化间隔的模型，由少量的支持向量决定。
	   
	2. SVM对于异常点不敏感，而LR敏感。SVM更加健壮，决策面不受非支持向量的影响。
	
	










	% \begin{enumerate}		
	% 	\item start with weights $w_i = \frac{1}{N} \quad i=1...N$
	% 	\item repeat for m=1 to M
	% 		\begin{itemize}
	% 			\item 使用输入数据训练一个分类器$G_m(x) \in (-1,1)$
	% 			\item 计算误差err:
	% 				\begin{equation*}
	% 					E_w[I_(y_i\not \equiv G_m(x_i))]=\frac{\sum_{i=1}^{N}w_i I_(y_i\not \equiv G_m(x_i))}{\sum_{i=1}^{N}w_i}
	% 				\end{equation*}
	% 			\item 输出$\alpha_m = \frac{1}{2}log(\frac{1-err}{err})$,从这里可以看出分类器的误差越大，权值越小
	% 			\item set $w_i = \frac{w_i}{Z_m} \exp(\alpha_m I_(y\not \equiv G_m(x))$，其中$Z_m$是规范化因子,\newline
	% 			$Z_m=\sum_{i=1}^{N}w_i \exp(-\alpha_m y_i G_m(x_i))$如果一个数据点分类错了，那么给这个点的权值大一点。
	% 		\end{itemize}
	% 	\item 这样我们就得到了$G(x)=sign[\sum_{m=1}^{M}\alpha_m G_m(x)]$
	% \end{enumerate}



%--------------------------------------------------------------------------------
%--------------------------------------------------------------------------------
%--------------------------------------------------------------------------------