\chapter{机器学习}

\section*{Introduction}
	本章节主要介绍一些常用的机器学习方法，部分内容参考《统计学习方法》李航
	

\section{决策树}
	
	\subsection{熵}
	熵，表示一个信号源发出的信号的不确定程度。在信源发出的信号中，某信号出现的概率越大，熵越小。熵只依赖于分布，而与具体的取值无关。如果用随机变量来表示的话，熵越大，表示随机变量的不确定性越大。
	
	香农使用log函数来定义样本i的信息熵：
	
	\begin{equation}
		f(p(i)) = \log \frac{1}{p(i)} = -\log p(i)
	\end{equation}
	
	因此在有n个样本的时候，我们用如下的公式来表示所有样本集合的信息熵：
	
	\begin{equation}
		E(P) = \sum_{i}^{n} \log p(i) \frac{1}{p(i)} = -\sum_{i}^{n} p(i)\log p(i)}
	\end{equation}
	
	然而，在机器学习中，我们通常使用模型分布q(i)来逼近真实分布p(i)，因此交叉熵的公式为：
	
	\begin{equation}
		E(P，Q) = -\sum_{i}^{n} p(i)\log q(i)}
	\end{equation}
	
	在tensorflow或者其他的深度学习框架中，我们使用的也是如上的公式来表示交叉熵。y表示网络的输出，y\_ 表示真是的label，公式如下：
	
	\begin{equation}
		cross\_entropy = -\sum_{i}^{n} y\_\log y}
	\end{equation}
	
	\subsection{信息增益}
	
	我们使用g(D,A)来表示特征A对数据集D的信息增益，E(D)表示数据集D的熵，E(D|A)表示在给定特征A的条件下D的经验熵。那么，我们可以使用如下的公式来表示信息增益：
	
	\begin{equation}
		g(D,A) = E(D) - E(D|A)
	\end{equation}
	
	E(D)表示对数据集D进行分类的不确定性，E(D|A)表示在给定特征A下对D分类的不确定性，那么它们的差就表示在使用了特征A后数据集信息不确定性的减少的程度。
	
	信息增益的大小是相对于数据集而言的，并没有绝对意义，因此信息增益比可以解决这个问题。信息增益比：
	
	\begin{equation}
		g_{R}(D,A) = \frac{g(D,A)}{E(D)}
	\end{equation}
	
	在具体的算法中如果需要计算信息增益或者信息增益比，只需要分别计算E(D),E(D|A)即可。计算$p_i$时只需要计算$C_i$这一类在整个数据集中出现的次数即可，其他相关的参数也是类似计算。
	
	\subsection{决策树生成}
	
	\subsubsection{ID3}
		对于训练数据D，特征集A，阈值$\epsilon$
		\begin{itemize}
			\item 如果D中的所有实例都属于同一类，则为一棵单节点树，算法结束。
			\item 如果特征集A为空，则T为单节点数，选择实例中出现次数最多的类作为该节点的类标记，算法结束
			\item 如果上述两条不满足，计算所有特征的信息增益，并选择信息增益最大的特征$A_k$，如果$A_k$小于阈值，则为单节点树，选择实例中出现次数最多的类作为该节点的类标记，算法结束。否则，对于$A_k$的每一个可能值$a_i$,对数据集D进行划分，这个划分就是这棵树的多个子树。
			\item 重复上述步骤即可得到一颗决策树。
		\end{itemize}
	\subsubsection{C4.5}
	C4.5算法与ID3的不同之处是使用信息增益比来进行特征的选择。
	
	\subsection{防止过拟合}
	
	决策树的生成过程只考虑局部最优，这样就会造成全局不是最优的结果。因此，剪枝的过程中需要考虑全局最优。
	
	决策树的剪枝通过最小化决策树的整体损失函数来实现。设决策树T有|T|个节点，t为树T的叶节点，该叶节点有$N_t$个样本点,其中属于k类的样本点有$N_{tk}$个，$E_t(T)$为叶节点的熵，$\alpha$为正则化参数，则决策树的损失函数定义为：
	
	\begin{equation}
	L = \sum _{t=1} ^{|T|}N_t E_t(T) + \alpha |T|
	\end{equation}
	
	决策树的损失函数表示的意思为，在保证整个数据集中样本点的熵的和尽量小的前提下，使得决策树中节点的数量更小。也就是说对于一些叶节点中样本的数量较少的节点，我们直接向上收缩，将这一个叶节点直接减掉，将其归到前一个分类条件中去。
	
	\subsection{CART算法}
	\subsubsection{回归树}
	\subsubsection{分类树}
	
\section{随机森林}




\section{GBDT}	
	
	
	
	
	
	
	