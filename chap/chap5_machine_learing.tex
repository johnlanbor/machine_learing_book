\chapter{机器学习}

\section*{Introduction}
	本章节主要介绍一些常用的机器学习方法，部分内容参考《统计学习方法》李航
	

\section{决策树}
	
	\subsection{熵}
	熵，表示一个信号源发出的信号的不确定程度。在信源发出的信号中，某信号出现的概率越大，熵越小。熵只依赖于分布，而与具体的取值无关。如果用随机变量来表示的话，熵越大，表示随机变量的不确定性越大。
	
	香农使用log函数来定义样本i的信息熵：
	
	\begin{equation}
		f(p(i)) = \log \frac{1}{p(i)} = -\log p(i)
	\end{equation}
	
	因此在有n个样本的时候，我们用如下的公式来表示所有样本集合的信息熵：
	
	\begin{equation}
		E(P) = \sum_{i}^{n} \log p(i) \frac{1}{p(i)} = -\sum_{i}^{n} p(i)\log p(i)}
	\end{equation}
	
	然而，在机器学习中，我们通常使用模型分布q(i)来逼近真实分布p(i)，因此交叉熵的公式为：
	
	\begin{equation}
		E(P，Q) = -\sum_{i}^{n} p(i)\log q(i)}
	\end{equation}
	
	在tensorflow或者其他的深度学习框架中，我们使用的也是如上的公式来表示交叉熵。y表示网络的输出，y\_ 表示真是的label，公式如下：
	
	\begin{equation}
		cross\_entropy = -\sum_{i}^{n} y\_\log y}
	\end{equation}
	
	\subsection{信息增益}
	
	我们使用g(D,A)来表示特征A对数据集D的信息增益，E(D)表示数据集D的熵，E(D|A)表示在给定特征A的条件下D的经验熵。那么，我们可以使用如下的公式来表示信息增益：
	
	\begin{equation}
		g(D,A) = E(D) - E(D|A)
	\end{equation}
	
	E(D)表示对数据集D进行分类的不确定性，E(D|A)表示在给定特征A下对D分类的不确定性，那么它们的差就表示在使用了特征A后数据集信息不确定性的减少的程度。
	
	信息增益的大小是相对于数据集而言的，并没有绝对意义，因此信息增益比可以解决这个问题。信息增益比：
	
	\begin{equation}
		g_{R}(D,A) = \frac{g(D,A)}{E(D)}
	\end{equation}
	
	在具体的算法中如果需要计算信息增益或者信息增益比，只需要分别计算E(D),E(D|A)即可。计算$p_i$时只需要计算$C_i$这一类在整个数据集中出现的次数即可，其他相关的参数也是类似计算。
	
	\subsection{决策树生成}
	
	\subsubsection{ID3}
		对于训练数据D，特征集A，阈值$\epsilon$
		\begin{itemize}
			\item 如果D中的所有实例都属于同一类，则为一棵单节点树，算法结束。
			\item 如果特征集A为空，则T为单节点数，选择实例中出现次数最多的类作为该节点的类标记，算法结束
			\item 如果上述两条不满足，计算所有特征的信息增益，并选择信息增益最大的特征$A_k$，如果$A_k$小于阈值，则为单节点树，选择实例中出现次数最多的类作为该节点的类标记，算法结束。否则，对于$A_k$的每一个可能值$a_i$,对数据集D进行划分，这个划分就是这棵树的多个子树。
			\item 重复上述步骤即可得到一颗决策树。
		\end{itemize}
	\subsubsection{C4.5}
	C4.5算法与ID3的不同之处是使用信息增益比来进行特征的选择。
	
	\subsection{防止过拟合}
	
	决策树的生成过程只考虑局部最优，这样就会造成全局不是最优的结果。因此，剪枝的过程中需要考虑全局最优。
	
	决策树的剪枝通过最小化决策树的整体损失函数来实现。设决策树T有|T|个节点，t为树T的叶节点，该叶节点有$N_t$个样本点,其中属于k类的样本点有$N_{tk}$个，$E_t(T)$为叶节点的熵，$\alpha$为正则化参数，则决策树的损失函数定义为：
	
	\begin{equation}
	L = \sum _{t=1} ^{|T|}N_t E_t(T) + \alpha |T|
	\end{equation}
	
	决策树的损失函数表示的意思为，在保证整个数据集中样本点的熵的和尽量小的前提下，使得决策树中节点的数量更小。也就是说对于一些叶节点中样本的数量较少的节点，我们直接向上收缩，将这一个叶节点直接减掉，将其归到前一个分类条件中去。
	
	\subsection{CART算法}
	\subsubsection{回归树}
	\subsubsection{分类树}
	
\section{随机森林}
	随机森林在属性（列）和数据（行）的使用上进行随机化，生成许多分类树，再汇总分类树的结果。随机森林在运算量没有显著提高的基础上提高了精度。
	
	每棵决策树都只对部分数据进行决策，而随机采样可以保证有重复的样本被不同的决策树所分类，这样就可以对决策树的分类能力做出评价。
	
	随机森林与Bagging有些类似。以决策树为基本模型的bagging在每次bootstrap放回抽样后，产生一次决策树，抽多少样本就产生多少棵树，在生成这些树的时候没有进行过多的干预。随机森林也是基于bootstrap进行随机抽样，但是每个节点的变量都是在随机产生的变量中产生。因此，不但样本是随机的，连每个节点变量的产生都是随机的。
	
	随机森林的loss function：熵
	
	\subsection{算法过程}
	\begin{itemize}
		\item 首先是两个随机采样。对于每一棵决策树来说，都需要对行进行随机选择（有放回，可能会产生重复样本），然后进行列采样，即从所有的特征中抽取部分特征。这样每棵树所使用的数据都不是所有的数据。这两个随机性的加入使得随机森林不至于过拟合。（首先进行行采样，然后在每个节点分裂成子树的时候随机选择列，再计算信息增益等信息进行特征选择）
		\item 然后对对于每一棵决策树来说，都采用完全分裂的方式建立。建立的过程同上一节的内容
		\item 由于步骤一的两个随机性的加入，使得随机森利即使不进行剪枝也不会出现过拟合的现象。
		\item 将生成的多棵决策树组合成随机森林，用随机森林分类器对新的数据进行判别与分类。分类结果按照多数表决决定。
	\end{itemize}
	
	为什么要对特征进行随机采样？如果有一个特征和标签特别强相关，那么在选择划分特征的时候，如果不随机的从所用的特征中选取一些特征的话，那么每一次这个强相关的特征都会被选取，那么每棵树都会是一样的。
	
	
	\subsection{算法优缺点}
	\subsubsection{优点}
	\begin{itemize}
		\item 两个随机性的引入，使得随机森林不容易过拟合，同时抗噪声能力更强
		\item 在处理高维数据的时候，不需要进行特征选择。顺便还可以确定重要的变量。
		\item 对数据的适应能力强，既能处理离散数据又能处理连续数据,数据集不需要进行规范化，不需要预处理，对缺失数据，非平衡数据稳健。
		\item 训练速度快，可以得到变量重要性的排序。可以进行特征提取
		\item 简单调参即可，跑一组试验画个图即可决定树的棵树。
	\end{itemize}
	\subsubsection{缺点}
	\begin{itemize}
		\item 模型规模大，评估速度慢
		\item 难以解释清楚
	\end{itemize}
	
	

	
	
	
\section{GBDT}	
	GBDT(Gradient Boosting Decision Tree),是一种迭代决策树算法。该算大由多棵决策树组成，所有树的结论累加起来作为最终答案。
	
	\subsection{回归树}
	随机森林中的树都是分类树，主要用于处理分类的情况。GBDT中的树都是回归树，搞清楚这一点是非常重要的。
	
	
	\subsection{一些注意事项}
	\begin{itemize}
		\item 自动发现有效的特征，特征组合，用来作为LR中的特征
		\item 每个节点得到一个预测值，最小化平方误差
		\item 以CART作为基分类器，基尼系数越大，不确定性越高$Gini(D,A) = \frac{|D_1|}{|D|} Gini(D_1)+ \frac{|D_2|}{|D|} Gini(D_2) $
	\end{itemize}	



\section{支持向量机}
	\subsection*{Introduction}
	SVM的决策函数为$sign(wx+b)$
	
	\subsection{预备知识}
		\subsubsection{函数间隔}
		在SVM中，分类的决策边界为一个超平面$wx+b=0$，一般来说，一个点距离这个超平面的远近可以表示分类的确信程度，在超平面确定的情况下，$|wx+b|$可以表示点到超平面的距离。而$wx+b$的符号可以表示分类正确与否，即是否与y一致。因此，可以使用$y(wx+b)$用来表示分类的正确性与确定度，这就是函数间隔：
		
		\begin{equation}
			\hat{\gamma_i} = y_i(wx_i+b)
		\end{equation}
		
		但是函数间隔存在的一个致命的问题就是，当我们等比例的改变w和b的时候，函数间隔也在等比例的改变，但是此时我们用来分类的超平面却没有改变。这样当我们最大化间隔的时候将没有任何意义，因此我们需要对超平买你的法向量做一些约束，比如对w进行规范化，$\|w\|=1$,使得对于一个超平面来说，间隔是确定的，这样就成了几何间隔。
		
		\subsubsection{几何间隔}
		如上所述，我们可以得到几何间隔的定义。对于给定的数据集T，给定的超平面$wx+b=0$，样本点$(x_i,y_i)$到超平面的几何间隔为：
		
		\begin{equation}
			\gamma_i = y_i(\frac{w}{\|w\|})x_i + \frac{b}{\|w\|}
		\end{equation}
		
		由于我们在使用SVM的时候最有是要取最大的间隔，以上为每个点的几何间隔，因此我们需要得到所有点的几个间隔的最小值($\gamma = \min \gamma_i $)，我们只要使得这个最小值最大化，就可以获得我们的最终结果。$\gamma$为整个数据集的几何间隔，$\gamma_i$为第i个数据的几何间隔。
		
		从函数间隔和几何间隔的定义我们可以得到它们之间的关系，如下式所示，当$\|w\|=1$的时候，函数间隔和几何间隔是相等的，但是如果等比例的改变w和b，函数间隔等比例的改变，但是几何间隔不变。
		
		\begin{equation}
		\gamma = \frac{\hat{\gamma}}{\|w\|}
		\end{equation}
	\subsection{硬间隔最大化}
	我们的主要目的是获得一个分类平面，并且使得所有数据到这个平面的距离尽可能的远。即最大化几何间隔，使得数据集中的每个样本点到超平面的几何间隔最小是$\gamma$
	
	\begin{equation}
		\max_{w,b}\ \gamma
	\end{equation}
	
	\begin{equation}
		s.t.\ y_i(\frac{w}{\|w\|}x_i + \frac{b}{\|w\|}) \geq \gamma
	\end{equation}
	
	根据函数间隔与几何间隔的关系我们可以将上述公式转化为下面的公式：
	
	\begin{equation}
		\max_{w,b}\ \frac{\hat{\gamma}}{\|w\|}
	\end{equation}
	
	\begin{equation}
		s.t.\ y_i(wx_i+b) \geq \hat{\gamma}
	\end{equation}
	
	对于上面的公式来说，即使等比例的改变w和b，也不会对目标函数的优化造成影响，因为等比例改变w和b，虽然目标函数等比例的改变了，约束条件也会因此改变。也就是说，上式中函数间隔其实对结果没有任何作用。因此，我们可以使函数间隔为一个固定的值，在这里我们使$\hat{\gamma} = 1$,这样上述公式就简化了，同时我们将最大化问题改成最小化问题，就得到了下面的最优化问题。硬间隔最大化的SVM公式如下：
		
	\begin{equation}
		\min_{w,b}\ \frac{1}{2}\|w\|^{2}
	\end{equation}
	
	\begin{equation}
		s.t.\ y_i(wx_i+b) -1 \geq 0
	\end{equation}
	
	求解上述公式可以得到$w^*,b^*$，这样进一步我们就可以得到SVM分类的决策函数$f(x) = sign(w^*x+b^*)$
	
	\subsection{硬间隔最大化的对偶问题}
	通过求解对偶问题可以得到原始问题的解，（最优化中的对偶问题，本章不做详细的描述），这样做有两个有点，一方面是对偶问题往往更容易求解，一方面是自然的引入了核函数。
	
	首先我们需要构造拉格朗日函数，引入拉格朗日乘子$\alpha_i \geq 0$,定义拉格朗日函数为：
	
	\begin{equation}
	L(w,b,\alpha) = \frac{1}{2}\|w\|^2 + \sum_{i=1}^{N}\alpha_i(1-y_i(wx_i+b))
	\end{equation}
	
	现在问题等价于求解$\min_{w,b}\ \max_{\alpha} L(w,b,\alpha)$,等价于求解$\max_{\alpha} \ \min_{w,b} L(w,b,\alpha)$,那么我们首先需要求解$\min_{w,b} L(w,b,\alpha)$,求解这个式子需要令其对于w和b的偏导数等与0，即：
	
	\begin{equation}
		\frac{\partial_L}{\partial_w}=\|w\| - \sum \alpha_i x_i y_i = 0
	\end{equation}

	\begin{equation}
		\frac{\partial_L}{\partial_b}=\sum \alpha_i y_i = 0
	\end{equation}
	
	将上面的导数带回原式得到：
	
	\begin{equation}
	L(w,b,\alpha) = -\frac{1}{2}\|w\|^2 + \sum_{i=0}^{N} \alpha_i
	\end{equation}
	
	接下来只需要求$\max_{\alpha}L(w,b,\alpha)$,取负号即可直接转换成最小化问题，得到新的函数为：

    \begin{gather}
	\min_{\alpha} \frac{1}{2}\|w\|^2 - \sum_{i=0}^{N} \alpha_i \\ \nonumber
	=\min_{\alpha} \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i \alpha_j y_i y_j x_i x_j - \sum_{i=0}^{N} \alpha_i
	\end{gather}
	
	\begin{equation}
	s.t.\ \sum_{i=1}^{N} \alpha_i y_i = 0
	\end{equation}
	
	\begin{equation}
	\qquad \alpha_i \geq 0, \ i=1,2,...N
	\end{equation}
	
	可以根据上述三个公式求得$\alpha^*$，然后进一步求得$w^*,b^*$,最终求得原始问题的解。下面还有一个问题就是，我们求得的解究竟是不是原始问题的解，如果是的话需要满足如下的KKT条件：
	
	\begin{equation}
		\frac{\partial_{L(w^*,b^*,\alpha^*)}}{\partial_w} = 0
	\end{equation}

	\begin{equation}
		\frac{\partial_{L(w^*,b^*,\alpha^*)}}{\partial_b} = 0
	\end{equation}
	
	\begin{equation}
		\alpha_i^*(y_i(w^*x_i+b^*)-1)=0,i=1,2,...,N
	\end{equation}
	
	\begin{equation}
		y_i(w^*x_i+b^*)-1 \geq 0,i=1,2,...,N
	\end{equation}
	
	\begin{equation}
		\alpha_i^* \geq 0,i=1,2,...,N
	\end{equation}
	
	如果满足如上的KKT条件，我们就可以根据对偶问题来求原始问题的解了。有对偶问题，由公式4.21-23我们可以求得$\alpha^*$,进一步我们可以求得$w^*,b^*$，这样我们就获得了整个原始问题中的参数。完成整个问题的求解
	
	\subsection{软间隔最大化}
	以上的问题仅仅是对于线性可分的情况下的训练数据，但是对于线性不可分的问题，以上的方法是不能很好的奏效的。软间隔最大化将以上方法扩展到了线性不可分的情况下了。并且这样做，可以避免SVM过拟合。
	
	线性不可分意味着有时候某些样本点是不能满足函数间隔大于1的要求的，因此为了解决这个问题，对于每一个样本点$(x_i,y_i)$,我们引入了一个松弛变量$\xi_i \geq 0$,使得函数间隔加上松弛变量大于等于1，，即我们的约束条件变成了如下的公式：
	
	\begin{equation}
	y_y(wx_i+b) \geq 1-\xi_i
	\end{equation}
	
	这样原始的线性不可分的线性支持向量机的问题，变成了如下的凸二次规划的问题（原始问题）：
	
	\begin{equation}
		\max_{w,b，\xi}\ \frac{1}{2}\|w\|^2 +C\sum_{i=1}^{N}\xi_i
	\end{equation}
	
	\begin{equation}
		s.t.\ y_y(wx_i+b) \geq 1-\xi_i, i=1,...N
	\end{equation}
	
	\begin{equation}
		\xi_i \geq 0, i=1,...,N
	\end{equation}
	
	下面我们给出以上问题的对偶问题，首先需要构造拉格朗日函数：
	
	\begin{equation}
	L(w,b,\alpha) = \frac{1}{2}\|w\|^2 +C\sum_{i=1}^{N}\xi_i +\sum_{i=1}^{N}\alpha_i(1-y_i(wx_i+b)-\xi_i) - \sum_{i=1}^{N}\mu_i\xi_i
	\end{equation}
	
	对偶问题是拉格朗日函数的极大极小问题，由上面我们知道，首先要求函数的极小，因此我们要对$w,b,\xi_i$,来分别求偏导：
	
	\begin{equation}
		\frac{\partial_L}{\partial_w}=\|w\| - \sum \alpha_i x_i y_i = 0
	\end{equation}

	\begin{equation}
		\frac{\partial_L}{\partial_b}=\sum \alpha_i y_i = 0
	\end{equation}
	
	\begin{equation}
		\frac{\partial_L}{\partial_{\xi_i}}= C-\alpha_i - \mu_i = 0
	\end{equation}
	
	将上述三个公式带入原问题，我们可以得到原始问题的对偶问题：
	
	\begin{gather}
	\min_{\alpha} \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i \alpha_j y_i y_j x_i x_j - \sum_{i=0}^{N} \alpha_i
	\end{gather}
	
	\begin{equation}
	s.t.\ \sum_{i=1}^{N} \alpha_i y_i = 0
	\end{equation}
	
	\begin{equation}
	0 \leq \alpha_i \leq C, \ i=1,2,...N
	\end{equation}
	
	当对偶问题满足如下的KKT条件时，我们就可以只求出如上对偶问题的解，来作为原始问题的解：
	
	\begin{equation}
		\frac{\partial_{L(w^*,b^*,\xi^*,\alpha^*,\mu^*)}}{\partial_w} = 0
	\end{equation}

	\begin{equation}
		\frac{\partial_{L(w^*,b^*,\xi^*,\alpha^*,\mu^*)}}{\partial_b} = 0
	\end{equation}
	
	\begin{equation}
		\frac{\partial_{L(w^*,b^*,\xi^*,\alpha^*,\mu^*)}}{\partial_{\xi}} = 0
	\end{equation}
	
	\begin{equation}
		\alpha_i^*(y_i(w^*x_i+b^*)-1 + \xi^*_i)=0,i=1,2,...,N
	\end{equation}
	
	\begin{equation}
		\mu_i^* \xi_i^* = 0
	\end{equation}
	
	\begin{equation}
		y_i(w^*x_i+b^*)-1 + \xi_i^* \geq 0,i=1,2,...,N
	\end{equation}
	
	\begin{equation}
		\alpha_i^* \geq 0,i=1,2,...,N
	\end{equation}
	
	\begin{equation}
		\xi_i^* \geq 0,i=1,2,...,N
	\end{equation}
	
	\begin{equation}
		\mu_i^* \geq 0,i=1,2,...,N
	\end{equation}
	
	当对偶问题满足如上条件后，我们可以根据对偶问题求得一个最优解$\alpha^*=(\alpha_i^*,...\alpha_N^*)$,然后可以进一步计算出$w^*,b^*$,这样我们就可以获得软间隔最大化的线性支持向量机的决策函数了。
	
	
	\subsection{非线性支持向量机}
	以上所有的内容，均为线性支持向量机，即使对于线性不可分的数据来说。
	
	非线性分类问题是指通过利用非线性模型才能很好的进行分类的问题，非线性支持向量机主要解决非线性的分类问题。
	
	\subsubsection{核函数}
	核函数的定义为:$K(x,z)=\phi(x)\cdot\phi(z)$
	
	核函数的想法是，在学习和预测时只使用核函数$K(x,z)$,而不显式的定义映射函数$\phi$。通常来说，直接计算核函数$K(x,z)$会更容易，但是通过$\phi(x)\cdot\phi(z)$来计算核函数缺并不容易。在软间隔最大化的时候我们可以使用核函数来避免使用，如：
	
	\begin{gather}
	\min_{\alpha} \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i \alpha_j y_i y_j K(x_i,x_j) - \sum_{i=0}^{N} \alpha_i
	\end{gather}
	
	\subsubsection{正定核}
	这一部分的内容，有需要再进行补充
	
	
	
	\subsection{一些注意事项}
	\begin{itemize}
		\item SVM如何防止过拟合？松弛变量的加入，即软间隔最大化
		\item 为什么要转化为对偶问题？因为原问题是凸二次规划问题，转化为对偶问题更加高效。
	\end{itemize}
	
	
	



\section{AdaBoost}
	本章内容来自于网络以及张志华老师的就《机器学习导论》课程

	本章基本内容就是给定一堆弱分类器，然后通过各种组合组成一个人强的分类器
	\subsection{离散的AdaBoost}
	离散的AdaBoost算法步骤：\boldmath  %公式加粗

	$w$表示给数据的权值，$\alpha$表示给分类器的权值

	\begin{enumerate}		
		\item start with weights $w_i = \frac{1}{N} \quad i=1...N$
		\item repeat for m=1 to M
			\begin{itemize}
				\item 使用输入数据训练一个分类器$G_m(x) \in (-1,1)$
				\item 计算误差err:
					\begin{equation*}
						E_w[I_(y_i\not \equiv G_m(x_i))]=\frac{\sum_{i=1}^{N}w_i I_(y_i\not \equiv G_m(x_i))}{\sum_{i=1}^{N}w_i}
					\end{equation*}
				\item 输出$\alpha_m = \frac{1}{2}log(\frac{1-err}{err})$,从这里可以看出分类器的误差越大，权值越小
				\item set $w_i = \frac{w_i}{Z_m} \exp(\alpha_m I_(y\not \equiv G_m(x))$，其中$Z_m$是规范化因子,\newline
				$Z_m=\sum_{i=1}^{N}w_i \exp(-\alpha_m y_i G_m(x_i))$如果一个数据点分类错了，那么给这个点的权值大一点。
			\end{itemize}
		\item 这样我们就得到了$G(x)=sign[\sum_{m=1}^{M}\alpha_m G_m(x)]$
	\end{enumerate}
	
	
	\subsection{Forward Stagest Additive Modeling}

	考虑加法模型(additive model)\boldmath
	\begin{equation}
		f(x)=\sum_{m=1}^{M}\beta_{m}b(x;\gamma_{m})
	\end{equation}
	其中,$b(x;\gamma_{m})$为基函数，$\gamma_m$为基函数的参数，$\beta_m$为基函数的系数。

	在给定训练数据以及Loss Function的情况下，相当于一个加法模型$f(x)$相当于一个minimize Loss Function的问题：
	\begin{equation}
		min_{\beta_m,\gamma_m} \sum_{i=1}^{N}L(y_i,\sum_{m=1}^{M}\beta_{m}b(x;\gamma_{m}))
	\end{equation}
	从前向后，每一步值学习一个基函数及其系数，即每一步只需优化如下的损失函数：
	\begin{equation}
		min_{\beta,\gamma} \sum_{i=1}^{N}L(y_i,\sum_{m=1}^{M}\beta b(x;\gamma))
	\end{equation}
	\newline
	\newline

	前向分布算法：

	输入：训练数据$T={(x_1,y_1),...(x_N,y_N)}$,Loss Function$L(x,f(x))$,基函数$b(x;\gamma)$

	输出：加法模型$f(x)$

	\begin{enumerate}		
		\item 初始化$f_0(x)=0$
		\item repeat for m=1 to M
			\begin{itemize}
				\item 计算参数$\beta_m,\gamma_m$
				\begin{equation}
					(\beta_m,\gamma_m)=\arg\min_{\beta,\gamma} \sum_{i=1}^{N}L(y_i,f_{m-1}(x_i)\beta b(x;\gamma))
				\end{equation}
				\item 更新$f_m(x)=f_{m-1}(x)+\beta_m b(x;\gamma_m)$
			\end{itemize}
		\item 得到加法模型
				\begin{equation}
					f(x)=\sum_{m=1}^{M} \beta_m b(x;\gamma_m)
				\end{equation}
	\end{enumerate}
	
	